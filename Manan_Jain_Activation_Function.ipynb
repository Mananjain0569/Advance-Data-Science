{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtBbTeeo9Q2t"
      },
      "source": [
        "# **Binary Step Function**   \n",
        "f(x) = 1, x>=0,    \n",
        "x<0  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lm_K_XOE9EOE",
        "outputId": "e2b8e376-244c-4c8a-c702-70b620b8996c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def binary_step(x):\n",
        "  if x<0:\n",
        "    return 0\n",
        "  else:\n",
        "    return 1\n",
        "binary_step(-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qC_fSxgQ9xtE"
      },
      "source": [
        "# **Linear Function**\n",
        "f(x)=ax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhOHUfUb9vjP",
        "outputId": "ae883940-0e63-4731-a726-aa71a4dfc32e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(16, -8)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def linear_function(x):\n",
        "  return 4*x\n",
        "linear_function(4), linear_function(-2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWRpRgnL-Aw0"
      },
      "source": [
        "# **Sigmoid**\n",
        "The next activation function that we are going to look at is the Sigmoid function. It is one of the most widely used non-linear activation function. Sigmoid transforms the values between the range 0 and 1. Here is the mathematical expression for sigmoid- f(x) = 1/(1+e^-x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bila4Wzv965S",
        "outputId": "dba00c53-f569-4ba2-b849-0c4f1e8f7405"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.9990889488055994, 2.7894680920908113e-10)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "def sigmoid_function(x):\n",
        "  z = (1/(1 + np.exp(-x)))\n",
        "  return z\n",
        "sigmoid_function(7),sigmoid_function(-22)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUvMIsfD-bk-"
      },
      "source": [
        "# **Tanh**\n",
        "The tanh function is very similar to the sigmoid function. The only difference is that it is symmetric around the origin. The range of values in this case is from -1 to 1. Thus the inputs to the next layers will not always be of the same sign. The tanh function is defined as- tanh(x)=2sigmoid(2x)-1\n",
        "\n",
        "tanh(x) = 2/(1+e^(-2x)) -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Izsop0nl-fN0",
        "outputId": "0bcd1157-7498-49c2-f407-037d47ffe913"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.4621171572600098, -0.7615941559557649)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def tanh_function(x):\n",
        "  z = (2/(1 + np.exp(-2*x))) -1\n",
        "  return z\n",
        "tanh_function(0.5), tanh_function(-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKkERv31-kVo"
      },
      "source": [
        "# **ReLU - Rectified Linear Unit**\n",
        "The ReLU function is another non-linear activation function. The main advantage is that it does not activate all the neurons at the same time.\n",
        "This means that the neurons will only be deactivated if the output of the linear transformation is less than 0.\n",
        "f(x)=max(0,x)\n",
        "Negative input values --> the result is zero, that means the neuron does not get activated.\n",
        "Since only a certain number of neurons are activated, the ReLU function is far more computationally efficient when compared to the sigmoid and tanh function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvZ5vHZi-hXL",
        "outputId": "f5d1a0fd-6595-4ab7-c160-d8d3ce1c2994"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(7, 0)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def relu_function(x):\n",
        "  if x<0:\n",
        "    return 0\n",
        "  else:\n",
        "    return x\n",
        "relu_function(7), relu_function(-7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1BShxK9-tMX"
      },
      "source": [
        "# **Leaky ReLU**\n",
        "Leaky ReLU function is an improved version of the ReLU function. As we saw that for the ReLU function, the gradient is 0 for x<0, which would deactivate the neurons in that region. Leaky ReLU is defined to address this problem. Instead of defining the Relu function as 0 for negative values of x, we define it as an extremely small linear component of x.\n",
        "\n",
        "Here is the mathematical expression-\n",
        "f(x)= 0.01x, x<0 = x, x>=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quZ3bCEV-qY4",
        "outputId": "1a6f284a-c2b3-4e88-d156-610a27fd5d50"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(7, -0.07)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def leaky_relu_function(x):\n",
        "  if x<0:\n",
        "    return 0.01*x\n",
        "  else:\n",
        "    return x\n",
        "leaky_relu_function(7), leaky_relu_function(-7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI5r0MZN-1tP"
      },
      "source": [
        "# **Parameterised ReLU**\n",
        "The parameterised ReLU introduces a new parameter as a slope of the negative part of the function.\n",
        "\n",
        "Here’s how the ReLU function is modified to incorporate the slope parameter- f(x) = x, x>=0\n",
        "\n",
        "= ax, x<0\n",
        "\n",
        "When the value of a is fixed to 0.01, the function acts as a Leaky ReLU function. However, in case of a parameterised ReLU function, ‘a‘ is also a trainable parameter . The network also learns the value of ‘a‘ for faster and more optimum convergence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvlrYlrK-4JK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6u2UUNaR-3bA"
      },
      "source": [
        "# **Exponential Linear Unit - ELU**\n",
        "Exponential Linear Unit a variant of Rectiufied Linear Unit (ReLU) that modifies the slope of the negative part of the function. Unlike the leaky relu and parametric ReLU functions, instead of a straight line, ELU uses a log curve for defning the negatice values. It is defined as\n",
        "\n",
        "f(x) = x, x>=0 = a(e^x-1), x<0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHI48NNd-xad",
        "outputId": "03fef012-7030-4d96-acbc-3742d4dc2e72"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5, -0.09932620530009145)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def elu_function(x, a):\n",
        "  if x<0:\n",
        "    return a*(np.exp(x)-1)\n",
        "  else:\n",
        "    return x\n",
        "elu_function(5, 0.1),elu_function(-5, 0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9Ecg3TM_DKg"
      },
      "source": [
        "# **Swish**\n",
        "Swish is a lesser known activation function which was discovered by researchers at Google.\n",
        "Swish is as computationally efficient as ReLU and shows better performance than ReLU on\n",
        "deeper models.\n",
        " Swish\n",
        "11/5/24, 3:52 PM 1_ActivationFunctions.ipynb - Colab\n",
        "\n",
        "https://colab.research.google.com/drive/1h01xKkit2FXoUXtbnJEC8ot7BQWdVBu6#printMode=true 4/9\n",
        "\n",
        "The values for swish ranges from negative infinity to infinity.\n",
        "The function is defined as –\n",
        "\n",
        "f(x) = x*sigmoid(x)\n",
        "f(x) = x/(1-e^-x)\n",
        "A unique fact about this function is that swich function is not monotonic.\n",
        "This means that the value of the function may decrease even when the input values are\n",
        "increasing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yzzyk1CG_Af8",
        "outputId": "0d477ec3-be5a-469e-b2b1-5d3461e88e83"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5.349885844610276e-28, 4.074629441455096)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def swish_function(x):\n",
        "  return x/(1-np.exp(-x))\n",
        "swish_function(-67), swish_function(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5GufDuH_KmB"
      },
      "source": [
        "# **Softmax**\n",
        "Softmax function is often described as a combination of multiple sigmoids.\n",
        "We know that sigmoid returns values between 0 and 1, which can be treated as probabilities of a\n",
        "data point belonging to a particular class.\n",
        "Thus sigmoid is widely used for binary classification problems.\n",
        "The softmax function can be used for multiclass classification problems.\n",
        "This function returns the probability for a datapoint belonging to each individual class.\n",
        "\n",
        "Here is the mathematical expression of the same-\n",
        "While building a network for a multiclass problem, the output layer would have as many neurons\n",
        "\n",
        "as the number of classes in the target.\n",
        "For instance if you have three classes, there would be three neurons in the output layer.\n",
        "Suppose you got the output from the neurons as [1.2 , 0.9 , 0.75].\n",
        "Applying the softmax function over these values, you will get the following result – [0.42 , 0.31,\n",
        "0.27]\n",
        ". These represent the probability for the data point belonging to each class.\n",
        "Note that the sum of all the values is 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vE_vadij_H48",
        "outputId": "17c6f38f-da69-432b-b636-fbf434c806ca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.08021815, 0.11967141, 0.80011044])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def softmax_function(x):\n",
        "  z = np.exp(x)\n",
        "  z_ = z/z.sum()\n",
        "  return z_\n",
        "softmax_function([0.8, 1.2, 3.1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5riWvcK__8V_"
      },
      "source": [
        "# Choosing the right Activation Function\n",
        "Good or bad – there is no rule of thumb.\n",
        "However depending upon the properties of the problem we might be able to make a better    \n",
        "choice for easy and quicker convergence of the network.   \n",
        "--> Sigmoid functions and their combinations generally work better in the case of classifiers    \n",
        "--> Sigmoids and tanh functions are sometimes avoided due to the vanishing gradient problem    \n",
        "--> ReLU function is a general activation function and is used in most cases these days    \n",
        "--> If we encounter a case of dead neurons in our networks the leaky ReLU function is the best choice       \n",
        "--> Always keep in mind that ReLU function should only be used in the hidden layers        \n",
        "--> As a rule of thumb, you can begin with using ReLU function and then move over to other        \n",
        "activation functions in case ReLU doesn’t provide with optimum results            "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_TcN6AhAZLO"
      },
      "source": [
        "All Activation Functions:    \n",
        "ReLU (Rectified Linear Unit)   \n",
        "Sigmoid  \n",
        "Tanh    \n",
        "Softmax   \n",
        "Leaky ReLU    \n",
        "ELU (Exponential Linear Unit)    \n",
        "Swish   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opRyYI_i_7w_"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gwwOwXAAAqsN",
        "outputId": "919795b4-37b7-4f8e-e8e7-2dc9e6a486d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Load the MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "# Normalize the images to the range [0, 1]\n",
        "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
        "# Flatten the images from 28x28 to 784-dimensional vectors\n",
        "X_train = X_train.reshape(-1, 784)\n",
        "X_test = X_test.reshape(-1, 784)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}