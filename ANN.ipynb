{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 1: Import Libraries**\n",
        "Import necessary libraries like:\n",
        "\n",
        "pandas for handling data,\n",
        "\n",
        "numpy for numerical operations,\n",
        "\n",
        "scikit-learn for preprocessing and evaluation."
      ],
      "metadata": {
        "id": "kNl9YXvjbxxg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "ftdmZW_BVHI_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Load and Clean the Dataset\n",
        "Load the CSV file: autism_screening.csv"
      ],
      "metadata": {
        "id": "abu_e7ygb4so"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"autism_screening.csv\")"
      ],
      "metadata": {
        "id": "addbpCdWWlbc"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ApYNGCsReJHd",
        "outputId": "63799961-6ca6-4765-cf9c-a385ed2cde91"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 609 entries, 0 to 703\n",
            "Data columns (total 19 columns):\n",
            " #   Column           Non-Null Count  Dtype  \n",
            "---  ------           --------------  -----  \n",
            " 0   A1_Score         609 non-null    int64  \n",
            " 1   A2_Score         609 non-null    int64  \n",
            " 2   A3_Score         609 non-null    int64  \n",
            " 3   A4_Score         609 non-null    int64  \n",
            " 4   A5_Score         609 non-null    int64  \n",
            " 5   A6_Score         609 non-null    int64  \n",
            " 6   A7_Score         609 non-null    int64  \n",
            " 7   A8_Score         609 non-null    int64  \n",
            " 8   A9_Score         609 non-null    int64  \n",
            " 9   A10_Score        609 non-null    int64  \n",
            " 10  age              609 non-null    float64\n",
            " 11  gender           609 non-null    int64  \n",
            " 12  ethnicity        609 non-null    int64  \n",
            " 13  jundice          609 non-null    int64  \n",
            " 14  austim           609 non-null    int64  \n",
            " 15  used_app_before  609 non-null    int64  \n",
            " 16  result           609 non-null    float64\n",
            " 17  relation         609 non-null    int64  \n",
            " 18  Class/ASD        609 non-null    int64  \n",
            "dtypes: float64(2), int64(17)\n",
            "memory usage: 95.2 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Remove irrelevant columns: 'age_desc' and 'contry_of_res'**\n",
        "\n",
        "**Replace '?' with NaN and drop rows with missing values to clean the dataset.**"
      ],
      "metadata": {
        "id": "FPAdhr30cB0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop irrelevant columns\n",
        "df = df.drop(['age_desc', 'contry_of_res'], axis=1)\n",
        "\n",
        "# Replace '?' with NaN and drop missing values\n",
        "df.replace('?', np.nan, inplace=True)\n",
        "df.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "KRiDdSIvWzHG"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 3: Encode Categorical Features**\n",
        "All categorical columns are label-encoded using LabelEncoder, converting text into numerical values (like Yes → 1, No → 0)."
      ],
      "metadata": {
        "id": "kW9aW7VAcR8-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Step 4: Define Features and Target\n",
        "X: All features (input variables)\n",
        "\n",
        "y: Target column 'Class/ASD' (whether the person has ASD or not)"
      ],
      "metadata": {
        "id": "XcA0upoUccP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode all categorical columns\n",
        "label_encoders = {}\n",
        "for column in df.columns:\n",
        "    if df[column].dtype == 'object':\n",
        "        le = LabelEncoder()\n",
        "        df[column] = le.fit_transform(df[column])\n",
        "        label_encoders[column] = le\n",
        "\n",
        "# Features and target\n",
        "X = df.drop('Class/ASD', axis=1).values\n",
        "y = df['Class/ASD'].values.reshape(-1, 1)  # Reshape y for MLP"
      ],
      "metadata": {
        "id": "LfdfvHjbXBBl"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Split the Data\n",
        "We split the dataset into training (80%) and testing (20%) using train_test_split."
      ],
      "metadata": {
        "id": "Zfphqi3mcoSl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "vil3Z4qcXM_N"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Standardize the Data\n",
        "StandardScaler is used to bring all features to a similar scale (mean = 0, std = 1), which helps in faster training of neural networks."
      ],
      "metadata": {
        "id": "ntXyi7dUcujE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "CY3sRXchXTsh"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7: Define Activation Function\n",
        "We use the sigmoid function for both hidden and output layers:"
      ],
      "metadata": {
        "id": "_2cRuYa1cyev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Activation function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)"
      ],
      "metadata": {
        "id": "mb8_zRw8XaV5"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 8: Create the MLP Class\n",
        "This class handles:\n",
        "\n",
        "Initialization of weights & biases\n",
        "\n",
        "Forward pass (input → hidden → output)\n",
        "\n",
        "Training using backpropagation\n",
        "\n",
        "Prediction\n",
        "\n",
        "It’s a simple 2-layer neural network:\n",
        "\n",
        "Input layer\n",
        "\n",
        "1 Hidden layer with 5 neurons\n",
        "\n",
        "Output layer with 1 neuron (for binary classification)\n",
        "\n"
      ],
      "metadata": {
        "id": "Pdv2TM_Gc3C-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: MLP class\n",
        "class MLP:\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        self.weights1 = np.random.randn(input_dim, hidden_dim)\n",
        "        self.bias1 = np.zeros((1, hidden_dim))\n",
        "        self.weights2 = np.random.randn(hidden_dim, output_dim)\n",
        "        self.bias2 = np.zeros((1, output_dim))\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.z1 = X.dot(self.weights1) + self.bias1\n",
        "        self.a1 = sigmoid(self.z1)\n",
        "        self.z2 = self.a1.dot(self.weights2) + self.bias2\n",
        "        self.a2 = sigmoid(self.z2)\n",
        "        return self.a2\n",
        "\n",
        "    def train(self, X, y, learning_rate=0.01, epochs=100):\n",
        "        for epoch in range(epochs):\n",
        "            # Forward pass\n",
        "            output = self.forward(X)\n",
        "\n",
        "            # Compute error\n",
        "            error = output - y\n",
        "\n",
        "            # Backpropagation\n",
        "            d_output = error * sigmoid_derivative(output)\n",
        "            d_hidden = d_output.dot(self.weights2.T) * sigmoid_derivative(self.a1)\n",
        "\n",
        "            # Update weights and biases\n",
        "            self.weights2 -= self.a1.T.dot(d_output) * learning_rate\n",
        "            self.bias2 -= np.sum(d_output, axis=0, keepdims=True) * learning_rate\n",
        "            self.weights1 -= X.T.dot(d_hidden) * learning_rate\n",
        "            self.bias1 -= np.sum(d_hidden, axis=0, keepdims=True) * learning_rate\n",
        "\n",
        "    def predict(self, X):\n",
        "        output = self.forward(X)\n",
        "        return (output > 0.5).astype(int)"
      ],
      "metadata": {
        "id": "a8L4w0KLXhJk"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 9: Train the MLP\n",
        "The model is trained using gradient descent for 300 epochs with a learning rate of 0.1."
      ],
      "metadata": {
        "id": "0PBVVvnQc_zI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 10: Evaluate the Model\n",
        "Predictions are made on the test set.\n",
        "\n",
        "Accuracy is calculated using accuracy_score."
      ],
      "metadata": {
        "id": "0JHqkDg0dDRh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10: Train and evaluate\n",
        "input_dim = X_train.shape[1]\n",
        "hidden_dim = 5\n",
        "output_dim = 1\n",
        "\n",
        "mlp_model = MLP(input_dim, hidden_dim, output_dim)\n",
        "mlp_model.train(X_train, y_train, learning_rate=0.1, epochs=300)\n",
        "\n",
        "y_pred = mlp_model.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\" MLP Model Accuracy:\", acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "sespR_gjXmCf",
        "outputId": "e885b0bb-b95e-4c10-d306-06cc6eb90ee9"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " MLP Model Accuracy: 0.9918032786885246\n"
          ]
        }
      ]
    }
  ]
}